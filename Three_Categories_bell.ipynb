{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad0b3a4-f0f8-4b09-b0c9-cab5dc79a3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 13:24:46.364469: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-02 13:24:46.399244: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-02 13:24:46.399270: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-02 13:24:46.400074: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-02 13:24:46.405699: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-02 13:24:47.067302: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from osgeo import gdal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pennylane as qml\n",
    "from keras.models import load_model\n",
    "from keras.utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout, Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "710bc194-c4d8-4839-87ef-7dc700778984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TiffImageDataGenerator(Sequence):\n",
    "    def __init__(self, image_files, labels, batch_size):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_files) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.image_files[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "\n",
    "        return np.array([\n",
    "            self.preprocess_image(file_name) for file_name in batch_x]), np.array(batch_y)\n",
    "\n",
    "    def preprocess_image(self, file):\n",
    "        dataset = gdal.Open(file)\n",
    "        channels = [dataset.GetRasterBand(i + 1).ReadAsArray() for i in range(dataset.RasterCount)]\n",
    "        image = np.stack(channels, axis=-1)\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        image = image / 255.0\n",
    "        return image\n",
    "\n",
    "dataset_path = \"/home/admin1/Selvin/BE/EuroSAT_MS/\"\n",
    "subdirs = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "image_files = []\n",
    "labels = []\n",
    "label = 0\n",
    "for subdir in subdirs:\n",
    "    image_files_subdir = [os.path.join(dataset_path, subdir, f) for f in os.listdir(os.path.join(dataset_path, subdir)) if f.endswith(\".tif\")]\n",
    "    image_files.extend(image_files_subdir)\n",
    "    labels.extend([label]*len(image_files_subdir))\n",
    "    label += 1\n",
    "\n",
    "# Define a dictionary to map the old labels to the new ones\n",
    "# Define a dictionary to map the old labels to the new ones\n",
    "label_mapping = {\n",
    "    'SeaLake': 'Water_Bodies',\n",
    "    'River': 'Water_Bodies',\n",
    "    'HerbaceousVegetation': 'Vegetation',\n",
    "    'PermanentCrop': 'Vegetation',\n",
    "    'AnnualCrop': 'Vegetation',\n",
    "    'Pasture': 'Vegetation',\n",
    "    'Forest': 'Vegetation',\n",
    "    'Industrial': 'Urban',\n",
    "    'Highway': 'Urban',\n",
    "    'Residential': 'Urban'\n",
    "}\n",
    "\n",
    "# Update the labels\n",
    "new_labels = []\n",
    "image_files = []  # Initialize the image_files list\n",
    "for subdir in subdirs:\n",
    "    image_files_subdir = [os.path.join(dataset_path, subdir, f) for f in os.listdir(os.path.join(dataset_path, subdir)) if f.endswith(\".tif\")]\n",
    "    image_files.extend(image_files_subdir)\n",
    "    new_labels.extend([label_mapping[subdir]]*len(image_files_subdir))\n",
    "\n",
    "# Now, you can use `new_labels` in place of `labels` for your train-test split and data generator\n",
    "# Define a dictionary to map the new string labels to numerical labels\n",
    "str_to_num_mapping = {\n",
    "    'Water_Bodies': 0,\n",
    "    'Vegetation': 1,\n",
    "    'Urban': 2\n",
    "}\n",
    "\n",
    "# Convert the string labels to numerical labels\n",
    "num_labels = [str_to_num_mapping[label] for label in new_labels]\n",
    "\n",
    "# Now, you can use `num_labels` in place of `new_labels` for your train-test split and data generator\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_files, num_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TiffImageDataGenerator instance for training and testing data\n",
    "train_gen = TiffImageDataGenerator(X_train, y_train, batch_size=32)\n",
    "test_gen = TiffImageDataGenerator(X_test, y_test, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a4f57e-2860-47cb-b362-1466e1927762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pennylane as qml\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class QuantumBellman(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(QuantumBellman, self).__init__(**kwargs)\n",
    "        self.dev = qml.device(\"default.qubit\", wires=4)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(QuantumBellman, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Convert inputs to tensor\n",
    "        inputs = tf.convert_to_tensor(inputs)\n",
    "        # Compute quantum circuit results\n",
    "        output = tf.vectorized_map(self.quantum_func, inputs)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 16)\n",
    "\n",
    "    def quantum_func(self, inputs):\n",
    "        @qml.qnode(self.dev, interface='tf')\n",
    "        def quantum_circuit(params):\n",
    "            # Hadamard gate to put qubits in superposition\n",
    "            qml.Hadamard(wires=0)\n",
    "\n",
    "            # Successive CNOT gates to entangle qubits\n",
    "            qml.CNOT(wires=[0, 1])\n",
    "            qml.CNOT(wires=[1, 2])\n",
    "            qml.CNOT(wires=[2, 3])\n",
    "\n",
    "            # Rotation around the y-axis using the parameters Î¸\n",
    "            for i in range(4):\n",
    "                qml.RY(params[i], wires=i)\n",
    "\n",
    "            # Reverse the CNOT operations\n",
    "            qml.CNOT(wires=[2, 3])\n",
    "            qml.CNOT(wires=[1, 2])\n",
    "            qml.CNOT(wires=[0, 1])\n",
    "\n",
    "            return qml.probs(wires=[0, 1, 2, 3])\n",
    "\n",
    "        # Compute quantum circuit results\n",
    "        probs = quantum_circuit(inputs)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bbea8a4-b919-4527-b157-bd4f8fb7658d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 13:24:48.743842: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-02 13:24:48.776604: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-02 13:24:48.780238: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-02 13:24:48.785383: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-02 13:24:48.789023: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-02 13:24:48.792498: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-02 13:24:48.918841: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-02 13:24:48.920456: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-02 13:24:48.921960: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-02 13:24:48.923336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1119 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 13)))  # Assuming your images have 3 channels\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(QuantumBellman())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(3, activation='softmax'))  # Assuming you have 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b11eb11-72d5-4cb3-a20e-719c214dc682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin1/miniconda3/lib/python3.11/site-packages/osgeo/gdal.py:312: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 13:24:52.154521: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-04-02 13:24:52.254666: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.256090: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:225] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.6\n",
      "2024-04-02 13:24:52.256113: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:228] Used ptxas at ptxas\n",
      "2024-04-02 13:24:52.256167: W external/local_xla/xla/stream_executor/gpu/redzone_allocator.cc:322] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2024-04-02 13:24:52.472696: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.474116: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:52.566295: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.566675: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.567056: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.567582: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.567902: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:52.568062: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:52.568163: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.568405: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:52.568908: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:52.569442: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:52.570162: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.570335: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.571583: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:52.571605: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:52.572736: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.573888: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:52.773252: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.774685: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:52.925413: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.925491: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.925768: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.927290: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:52.927329: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:52.927390: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:52.927503: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.927748: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.928845: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.929083: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:52.929107: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:52.929322: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.930117: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:52.930610: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:52.931257: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.931691: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:52.932428: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:52.933018: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:53.465434: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:53.465625: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:53.467298: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:53.467610: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:53.467645: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:53.467679: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:53.468838: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:53.469667: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:53.469704: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:53.470039: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:53.470576: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:53.470758: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:53.471148: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:53.472296: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:53.472606: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:53.472977: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:53.473304: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:53.475252: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:53.885679: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:53.887343: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:53.924673: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:53.926104: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:53.969410: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:53.970827: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:54.003667: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fbaf40256e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-02 13:24:54.003693: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A4000, Compute Capability 8.6\n",
      "2024-04-02 13:24:54.008900: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-02 13:24:54.046003: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:54.047618: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:408] Couldn't read CUDA driver version.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712044494.072287   27058 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-04-02 13:24:54.135182: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:54.198093: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:54.252522: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:54.254012: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:24:54.356618: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:54.517012: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:54.666004: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:54.789309: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:54.891710: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:55.002685: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:55.141940: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:55.537056: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:55.670993: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:55.884885: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:24:56.017958: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/675 [..............................] - ETA: 12s - loss: 1.2852 - accuracy: 0.3875    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 13:24:56.216569: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/675 [============================>.] - ETA: 0s - loss: 0.4401 - accuracy: 0.8424 Epoch 1/30 -  Training Loss: 0.4393,  Validation Loss: 0.4724,  Training Accuracy: 84.27%,  Validation Accuracy: 79.93%\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.79926, saving model to best_model.h5\n",
      "675/675 [==============================] - 32s 37ms/step - loss: 0.4393 - accuracy: 0.8427 - val_loss: 0.4724 - val_accuracy: 0.7993\n",
      "Epoch 2/30\n",
      "  1/675 [..............................] - ETA: 42s - loss: 0.1268 - accuracy: 0.9688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin1/miniconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674/675 [============================>.] - ETA: 0s - loss: 0.2003 - accuracy: 0.9365 Epoch 2/30 -  Training Loss: 0.2001,  Validation Loss: 0.1918,  Training Accuracy: 93.66%,  Validation Accuracy: 93.43%\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.79926 to 0.93426, saving model to best_model.h5\n",
      "675/675 [==============================] - 24s 36ms/step - loss: 0.2001 - accuracy: 0.9366 - val_loss: 0.1918 - val_accuracy: 0.9343\n",
      "Epoch 3/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.1560 - accuracy: 0.9536 Epoch 3/30 -  Training Loss: 0.1558,  Validation Loss: 0.2392,  Training Accuracy: 95.37%,  Validation Accuracy: 90.98%\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.93426\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.1558 - accuracy: 0.9537 - val_loss: 0.2392 - val_accuracy: 0.9098\n",
      "Epoch 4/30\n",
      "673/675 [============================>.] - ETA: 0s - loss: 0.1360 - accuracy: 0.9583 Epoch 4/30 -  Training Loss: 0.1361,  Validation Loss: 1.0720,  Training Accuracy: 95.84%,  Validation Accuracy: 70.94%\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.93426\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.1361 - accuracy: 0.9584 - val_loss: 1.0720 - val_accuracy: 0.7094\n",
      "Epoch 5/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1125 - accuracy: 0.9666 Epoch 5/30 -  Training Loss: 0.1125,  Validation Loss: 1.1886,  Training Accuracy: 96.66%,  Validation Accuracy: 67.04%\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.93426\n",
      "675/675 [==============================] - 26s 38ms/step - loss: 0.1125 - accuracy: 0.9666 - val_loss: 1.1886 - val_accuracy: 0.6704\n",
      "Epoch 6/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1034 - accuracy: 0.9700 Epoch 6/30 -  Training Loss: 0.1034,  Validation Loss: 0.0704,  Training Accuracy: 97.00%,  Validation Accuracy: 97.76%\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.93426 to 0.97759, saving model to best_model.h5\n",
      "675/675 [==============================] - 26s 38ms/step - loss: 0.1034 - accuracy: 0.9700 - val_loss: 0.0704 - val_accuracy: 0.9776\n",
      "Epoch 7/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1020 - accuracy: 0.9710 Epoch 7/30 -  Training Loss: 0.1020,  Validation Loss: 0.1651,  Training Accuracy: 97.10%,  Validation Accuracy: 93.67%\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.97759\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.1020 - accuracy: 0.9710 - val_loss: 0.1651 - val_accuracy: 0.9367\n",
      "Epoch 8/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0913 - accuracy: 0.9739 Epoch 8/30 -  Training Loss: 0.0913,  Validation Loss: 0.5597,  Training Accuracy: 97.39%,  Validation Accuracy: 78.80%\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.97759\n",
      "675/675 [==============================] - 26s 38ms/step - loss: 0.0913 - accuracy: 0.9739 - val_loss: 0.5597 - val_accuracy: 0.7880\n",
      "Epoch 9/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0800 - accuracy: 0.9764 Epoch 9/30 -  Training Loss: 0.0799,  Validation Loss: 0.0884,  Training Accuracy: 97.64%,  Validation Accuracy: 97.20%\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.97759\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.0799 - accuracy: 0.9764 - val_loss: 0.0884 - val_accuracy: 0.9720\n",
      "Epoch 10/30\n",
      "673/675 [============================>.] - ETA: 0s - loss: 0.0787 - accuracy: 0.9770 Epoch 10/30 -  Training Loss: 0.0785,  Validation Loss: 0.0706,  Training Accuracy: 97.71%,  Validation Accuracy: 97.81%\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.97759 to 0.97815, saving model to best_model.h5\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.0785 - accuracy: 0.9771 - val_loss: 0.0706 - val_accuracy: 0.9781\n",
      "Epoch 11/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9831 Epoch 11/30 -  Training Loss: 0.0593,  Validation Loss: 0.0613,  Training Accuracy: 98.31%,  Validation Accuracy: 98.04%\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.97815 to 0.98037, saving model to best_model.h5\n",
      "675/675 [==============================] - 23s 34ms/step - loss: 0.0593 - accuracy: 0.9831 - val_loss: 0.0613 - val_accuracy: 0.9804\n",
      "Epoch 12/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0591 - accuracy: 0.9830 Epoch 12/30 -  Training Loss: 0.0590,  Validation Loss: 0.1445,  Training Accuracy: 98.30%,  Validation Accuracy: 95.17%\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.98037\n",
      "675/675 [==============================] - 24s 35ms/step - loss: 0.0590 - accuracy: 0.9830 - val_loss: 0.1445 - val_accuracy: 0.9517\n",
      "Epoch 13/30\n",
      "673/675 [============================>.] - ETA: 0s - loss: 0.0521 - accuracy: 0.9855 Epoch 13/30 -  Training Loss: 0.0521,  Validation Loss: 0.1706,  Training Accuracy: 98.54%,  Validation Accuracy: 94.28%\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.98037\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.0521 - accuracy: 0.9854 - val_loss: 0.1706 - val_accuracy: 0.9428\n",
      "Epoch 14/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0459 - accuracy: 0.9869 Epoch 14/30 -  Training Loss: 0.0460,  Validation Loss: 0.1096,  Training Accuracy: 98.69%,  Validation Accuracy: 96.76%\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.98037\n",
      "675/675 [==============================] - 26s 39ms/step - loss: 0.0460 - accuracy: 0.9869 - val_loss: 0.1096 - val_accuracy: 0.9676\n",
      "Epoch 15/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0433 - accuracy: 0.9879 Epoch 15/30 -  Training Loss: 0.0433,  Validation Loss: 0.0556,  Training Accuracy: 98.79%,  Validation Accuracy: 98.30%\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.98037 to 0.98296, saving model to best_model.h5\n",
      "675/675 [==============================] - 25s 36ms/step - loss: 0.0433 - accuracy: 0.9879 - val_loss: 0.0556 - val_accuracy: 0.9830\n",
      "Epoch 16/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9903 Epoch 16/30 -  Training Loss: 0.0336,  Validation Loss: 0.0505,  Training Accuracy: 99.03%,  Validation Accuracy: 98.46%\n",
      "\n",
      "Epoch 16: val_accuracy improved from 0.98296 to 0.98463, saving model to best_model.h5\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.0336 - accuracy: 0.9903 - val_loss: 0.0505 - val_accuracy: 0.9846\n",
      "Epoch 17/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0362 - accuracy: 0.9895 Epoch 17/30 -  Training Loss: 0.0362,  Validation Loss: 0.0378,  Training Accuracy: 98.95%,  Validation Accuracy: 98.91%\n",
      "\n",
      "Epoch 17: val_accuracy improved from 0.98463 to 0.98907, saving model to best_model.h5\n",
      "675/675 [==============================] - 27s 40ms/step - loss: 0.0362 - accuracy: 0.9895 - val_loss: 0.0378 - val_accuracy: 0.9891\n",
      "Epoch 18/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9921 Epoch 18/30 -  Training Loss: 0.0282,  Validation Loss: 0.0861,  Training Accuracy: 99.21%,  Validation Accuracy: 97.61%\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.98907\n",
      "675/675 [==============================] - 27s 40ms/step - loss: 0.0282 - accuracy: 0.9921 - val_loss: 0.0861 - val_accuracy: 0.9761\n",
      "Epoch 19/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9925 Epoch 19/30 -  Training Loss: 0.0262,  Validation Loss: 0.1298,  Training Accuracy: 99.25%,  Validation Accuracy: 95.67%\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.98907\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.0262 - accuracy: 0.9925 - val_loss: 0.1298 - val_accuracy: 0.9567\n",
      "Epoch 20/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9921 Epoch 20/30 -  Training Loss: 0.0249,  Validation Loss: 0.0926,  Training Accuracy: 99.21%,  Validation Accuracy: 97.57%\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.98907\n",
      "675/675 [==============================] - 27s 39ms/step - loss: 0.0249 - accuracy: 0.9921 - val_loss: 0.0926 - val_accuracy: 0.9757\n",
      "Epoch 21/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0223 - accuracy: 0.9936 Epoch 21/30 -  Training Loss: 0.0223,  Validation Loss: 0.4474,  Training Accuracy: 99.36%,  Validation Accuracy: 92.83%\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.98907\n",
      "675/675 [==============================] - 27s 39ms/step - loss: 0.0223 - accuracy: 0.9936 - val_loss: 0.4474 - val_accuracy: 0.9283\n",
      "Epoch 22/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0191 - accuracy: 0.9943 Epoch 22/30 -  Training Loss: 0.0191,  Validation Loss: 0.0672,  Training Accuracy: 99.43%,  Validation Accuracy: 98.06%\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.98907\n",
      "675/675 [==============================] - 26s 38ms/step - loss: 0.0191 - accuracy: 0.9943 - val_loss: 0.0672 - val_accuracy: 0.9806\n",
      "Epoch 23/30\n",
      "673/675 [============================>.] - ETA: 0s - loss: 0.0251 - accuracy: 0.9933 Epoch 23/30 -  Training Loss: 0.0252,  Validation Loss: 0.5118,  Training Accuracy: 99.32%,  Validation Accuracy: 87.59%\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.98907\n",
      "675/675 [==============================] - 27s 41ms/step - loss: 0.0252 - accuracy: 0.9932 - val_loss: 0.5118 - val_accuracy: 0.8759\n",
      "Epoch 24/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9954 Epoch 24/30 -  Training Loss: 0.0169,  Validation Loss: 0.0541,  Training Accuracy: 99.54%,  Validation Accuracy: 98.78%\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.98907\n",
      "675/675 [==============================] - 26s 38ms/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.0541 - val_accuracy: 0.9878\n",
      "Epoch 25/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9958 Epoch 25/30 -  Training Loss: 0.0150,  Validation Loss: 0.1211,  Training Accuracy: 99.58%,  Validation Accuracy: 96.76%\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.98907\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.0150 - accuracy: 0.9958 - val_loss: 0.1211 - val_accuracy: 0.9676\n",
      "Epoch 26/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0180 - accuracy: 0.9948 Epoch 26/30 -  Training Loss: 0.0180,  Validation Loss: 0.0867,  Training Accuracy: 99.48%,  Validation Accuracy: 97.83%\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.98907\n",
      "675/675 [==============================] - 28s 41ms/step - loss: 0.0180 - accuracy: 0.9948 - val_loss: 0.0867 - val_accuracy: 0.9783\n",
      "Epoch 27/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9964 Epoch 27/30 -  Training Loss: 0.0135,  Validation Loss: 0.1216,  Training Accuracy: 99.64%,  Validation Accuracy: 96.65%\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.98907\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.0135 - accuracy: 0.9964 - val_loss: 0.1216 - val_accuracy: 0.9665\n",
      "Epoch 27: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "epochs = 30\n",
    "\n",
    "# Define the ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    \"best_model.h5\",\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback to stop training if there's no improvement in validation accuracy\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "print_loss_accuracy_callback = LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: print(\n",
    "        f\" Epoch {epoch + 1}/{epochs} - \"\n",
    "        f\" Training Loss: {logs['loss']:.4f}, \"\n",
    "        f\" Validation Loss: {logs['val_loss']:.4f}, \"\n",
    "        f\" Training Accuracy: {logs['accuracy'] * 100:.2f}%, \"\n",
    "        f\" Validation Accuracy: {logs['val_accuracy'] * 100:.2f}%\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Train the model using the generators and include the callbacks\n",
    "history = model.fit(train_gen, validation_data=test_gen, epochs=epochs, callbacks=[print_loss_accuracy_callback, checkpoint_callback, early_stopping_callback])\n",
    "\n",
    "# model.fit(train_gen, validation_data=test_gen, epochs=epochs, callbacks=[print_loss_accuracy_callback, checkpoint_callback])\n",
    "\n",
    "# Load the best model based on validation accuracy\n",
    "best_model = load_model(\"best_model.h5\", custom_objects={'QuantumBellman': QuantumBellman})\n",
    "\n",
    "# Save the best model with a different name, for example, \"PQC_Circuit\"\n",
    "best_model.save(\"Three_Categories_Quantum_bell.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa2698c5-3bc9-4cb6-b05b-548ccc2f9297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 6s 34ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1100\n",
      "           1       0.99      0.99      0.99      2682\n",
      "           2       0.99      0.98      0.98      1618\n",
      "\n",
      "    accuracy                           0.99      5400\n",
      "   macro avg       0.99      0.99      0.99      5400\n",
      "weighted avg       0.99      0.99      0.99      5400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(test_gen)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce2e4b-a823-47dd-847b-9df15365847c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
