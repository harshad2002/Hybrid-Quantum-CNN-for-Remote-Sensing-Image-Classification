{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad0b3a4-f0f8-4b09-b0c9-cab5dc79a3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 10:31:02.601503: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-09 10:31:02.636650: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-09 10:31:02.636681: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-09 10:31:02.637545: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-09 10:31:02.643655: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-09 10:31:03.307416: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from osgeo import gdal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pennylane as qml\n",
    "from keras.models import load_model\n",
    "from keras.utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout, Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "710bc194-c4d8-4839-87ef-7dc700778984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TiffImageDataGenerator(Sequence):\n",
    "    def __init__(self, image_files, labels, batch_size):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_files) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.image_files[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "\n",
    "        return np.array([\n",
    "            self.preprocess_image(file_name) for file_name in batch_x]), np.array(batch_y)\n",
    "\n",
    "    def preprocess_image(self, file):\n",
    "        dataset = gdal.Open(file)\n",
    "        channels = [dataset.GetRasterBand(i + 1).ReadAsArray() for i in range(dataset.RasterCount)]\n",
    "        image = np.stack(channels, axis=-1)\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        image = image / 255.0\n",
    "        return image\n",
    "\n",
    "dataset_path = \"/home/admin1/Selvin/BE/EuroSAT_MS/\"\n",
    "subdirs = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "image_files = []\n",
    "labels = []\n",
    "label = 0\n",
    "for subdir in subdirs:\n",
    "    image_files_subdir = [os.path.join(dataset_path, subdir, f) for f in os.listdir(os.path.join(dataset_path, subdir)) if f.endswith(\".tif\")]\n",
    "    image_files.extend(image_files_subdir)\n",
    "    labels.extend([label]*len(image_files_subdir))\n",
    "    label += 1\n",
    "\n",
    "# Define a dictionary to map the old labels to the new ones\n",
    "# Define a dictionary to map the old labels to the new ones\n",
    "label_mapping = {\n",
    "    'SeaLake': 'Water_Bodies',\n",
    "    'River': 'Water_Bodies',\n",
    "    'HerbaceousVegetation': 'Vegetation',\n",
    "    'PermanentCrop': 'Vegetation',\n",
    "    'AnnualCrop': 'Vegetation',\n",
    "    'Pasture': 'Vegetation',\n",
    "    'Forest': 'Vegetation',\n",
    "    'Industrial': 'Urban',\n",
    "    'Highway': 'Urban',\n",
    "    'Residential': 'Urban'\n",
    "}\n",
    "\n",
    "# Update the labels\n",
    "new_labels = []\n",
    "image_files = []  # Initialize the image_files list\n",
    "for subdir in subdirs:\n",
    "    image_files_subdir = [os.path.join(dataset_path, subdir, f) for f in os.listdir(os.path.join(dataset_path, subdir)) if f.endswith(\".tif\")]\n",
    "    image_files.extend(image_files_subdir)\n",
    "    new_labels.extend([label_mapping[subdir]]*len(image_files_subdir))\n",
    "\n",
    "# Now, you can use `new_labels` in place of `labels` for your train-test split and data generator\n",
    "# Define a dictionary to map the new string labels to numerical labels\n",
    "str_to_num_mapping = {\n",
    "    'Water_Bodies': 0,\n",
    "    'Vegetation': 1,\n",
    "    'Urban': 2\n",
    "}\n",
    "\n",
    "# Convert the string labels to numerical labels\n",
    "num_labels = [str_to_num_mapping[label] for label in new_labels]\n",
    "\n",
    "# Now, you can use `num_labels` in place of `new_labels` for your train-test split and data generator\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_files, num_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TiffImageDataGenerator instance for training and testing data\n",
    "train_gen = TiffImageDataGenerator(X_train, y_train, batch_size=32)\n",
    "test_gen = TiffImageDataGenerator(X_test, y_test, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79a4f57e-2860-47cb-b362-1466e1927762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantum(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Quantum, self).__init__(**kwargs)\n",
    "        self.dev = qml.device(\"default.qubit\", wires=4)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Quantum, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Convert inputs to tensor\n",
    "        inputs = tf.convert_to_tensor(inputs)\n",
    "        # Compute quantum circuit results\n",
    "        output = tf.vectorized_map(self.quantum_func, inputs)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 16)\n",
    "\n",
    "    def quantum_func(self, inputs):\n",
    "        @qml.qnode(self.dev, interface='tf')\n",
    "        def quantum_circuit(params):\n",
    "            qml.Hadamard(wires=0)\n",
    "            qml.Hadamard(wires=1)\n",
    "            qml.Hadamard(wires=2)\n",
    "            qml.Hadamard(wires=3)\n",
    "\n",
    "            # Use the values in the batch item for the initial rotation (theta1)\n",
    "            for i in range(4):\n",
    "                qml.RY(params[i], wires=i)\n",
    "\n",
    "            qml.CNOT(wires=[0, 1])\n",
    "            qml.CNOT(wires=[0, 2])\n",
    "            qml.CNOT(wires=[0, 3])\n",
    "            qml.CNOT(wires=[1, 2])\n",
    "            qml.CNOT(wires=[1, 3])\n",
    "            qml.CNOT(wires=[2, 3])\n",
    "\n",
    "            # Use the next 4 values in the batch item for the second rotation (theta2)\n",
    "            for i in range(4, 8):\n",
    "                qml.RY(params[i], wires=i % 4)\n",
    "\n",
    "            return qml.probs(wires=[0, 1, 2, 3])\n",
    "\n",
    "        # Compute quantum circuit results\n",
    "        probs = quantum_circuit(inputs)\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bbea8a4-b919-4527-b157-bd4f8fb7658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 13)))  # Assuming your images have 3 channels\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Quantum())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(3, activation='softmax'))  # Assuming you have 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b11eb11-72d5-4cb3-a20e-719c214dc682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 10:38:03.146975: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-02-09 10:38:03.245601: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:03.246869: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:225] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.6\n",
      "2024-02-09 10:38:03.246891: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:228] Used ptxas at ptxas\n",
      "2024-02-09 10:38:03.246941: W external/local_xla/xla/stream_executor/gpu/redzone_allocator.cc:322] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2024-02-09 10:38:03.448493: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:03.450025: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:03.581020: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:03.581948: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:03.583035: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:03.583324: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:03.583706: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:03.583888: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:03.584621: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:03.585390: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:03.585506: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:03.585863: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:03.585969: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:03.585987: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:03.586216: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:03.587122: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:03.587133: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:03.587392: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:03.789570: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:03.792016: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.000597: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.000801: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.001115: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.002112: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.002125: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.002377: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.002518: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.003063: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.003568: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.003850: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.003879: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.004099: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.004453: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.004850: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.005069: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.005463: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.006208: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.007509: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.488145: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.488455: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.489406: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.490154: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.490473: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.490517: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.491318: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.492058: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.492342: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.492724: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.492829: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.493753: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.494962: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.494998: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.495037: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.495939: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.496689: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.499119: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.912642: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.914211: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:04.951266: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:04.952907: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:05.012973: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:05.014939: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:05.072055: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f3dac4c7790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-09 10:38:05.072095: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A4000, Compute Capability 8.6\n",
      "2024-02-09 10:38:05.080976: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-02-09 10:38:05.141348: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:05.144923: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:408] Couldn't read CUDA driver version.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1707455285.179085   10460 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-02-09 10:38:05.230604: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:05.288056: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:05.340026: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:05.341511: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-02-09 10:38:05.476593: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:05.652117: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:05.756045: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:05.860396: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:05.980941: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:06.084463: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:06.229715: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:06.644121: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:06.743505: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:06.992800: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-02-09 10:38:07.140441: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/675 [..............................] - ETA: 13s - loss: 1.2943 - accuracy: 0.3875    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 10:38:07.320733: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - ETA: 0s - loss: 0.5720 - accuracy: 0.7638 Epoch 1/50 -  Training Loss: 0.5720,  Validation Loss: 0.5914,  Training Accuracy: 76.38%,  Validation Accuracy: 81.04%\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.81037, saving model to best_model.h5\n",
      "675/675 [==============================] - 31s 35ms/step - loss: 0.5720 - accuracy: 0.7638 - val_loss: 0.5914 - val_accuracy: 0.8104\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin1/miniconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674/675 [============================>.] - ETA: 0s - loss: 0.1996 - accuracy: 0.9362 Epoch 2/50 -  Training Loss: 0.1993,  Validation Loss: 0.1830,  Training Accuracy: 93.62%,  Validation Accuracy: 93.37%\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.81037 to 0.93370, saving model to best_model.h5\n",
      "675/675 [==============================] - 24s 35ms/step - loss: 0.1993 - accuracy: 0.9362 - val_loss: 0.1830 - val_accuracy: 0.9337\n",
      "Epoch 3/50\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.1563 - accuracy: 0.9530 Epoch 3/50 -  Training Loss: 0.1562,  Validation Loss: 0.1590,  Training Accuracy: 95.31%,  Validation Accuracy: 94.78%\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.93370 to 0.94778, saving model to best_model.h5\n",
      "675/675 [==============================] - 23s 34ms/step - loss: 0.1562 - accuracy: 0.9531 - val_loss: 0.1590 - val_accuracy: 0.9478\n",
      "Epoch 4/50\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9613 Epoch 4/50 -  Training Loss: 0.1315,  Validation Loss: 0.4073,  Training Accuracy: 96.13%,  Validation Accuracy: 86.91%\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.94778\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.1315 - accuracy: 0.9613 - val_loss: 0.4073 - val_accuracy: 0.8691\n",
      "Epoch 5/50\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.1180 - accuracy: 0.9647 Epoch 5/50 -  Training Loss: 0.1178,  Validation Loss: 0.2652,  Training Accuracy: 96.47%,  Validation Accuracy: 91.54%\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.94778\n",
      "675/675 [==============================] - 25s 36ms/step - loss: 0.1178 - accuracy: 0.9647 - val_loss: 0.2652 - val_accuracy: 0.9154\n",
      "Epoch 6/50\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1056 - accuracy: 0.9687 Epoch 6/50 -  Training Loss: 0.1056,  Validation Loss: 0.5862,  Training Accuracy: 96.87%,  Validation Accuracy: 80.98%\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.94778\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.1056 - accuracy: 0.9687 - val_loss: 0.5862 - val_accuracy: 0.8098\n",
      "Epoch 7/50\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0961 - accuracy: 0.9721 Epoch 7/50 -  Training Loss: 0.0961,  Validation Loss: 0.0696,  Training Accuracy: 97.21%,  Validation Accuracy: 98.02%\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.94778 to 0.98019, saving model to best_model.h5\n",
      "675/675 [==============================] - 24s 35ms/step - loss: 0.0961 - accuracy: 0.9721 - val_loss: 0.0696 - val_accuracy: 0.9802\n",
      "Epoch 8/50\n",
      "673/675 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.9764 Epoch 8/50 -  Training Loss: 0.0821,  Validation Loss: 0.0884,  Training Accuracy: 97.64%,  Validation Accuracy: 97.00%\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.98019\n",
      "675/675 [==============================] - 23s 35ms/step - loss: 0.0821 - accuracy: 0.9764 - val_loss: 0.0884 - val_accuracy: 0.9700\n",
      "Epoch 9/50\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9800 Epoch 9/50 -  Training Loss: 0.0666,  Validation Loss: 0.2199,  Training Accuracy: 98.00%,  Validation Accuracy: 92.61%\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.98019\n",
      "675/675 [==============================] - 27s 40ms/step - loss: 0.0666 - accuracy: 0.9800 - val_loss: 0.2199 - val_accuracy: 0.9261\n",
      "Epoch 10/50\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0694 - accuracy: 0.9790 Epoch 10/50 -  Training Loss: 0.0694,  Validation Loss: 0.0718,  Training Accuracy: 97.90%,  Validation Accuracy: 97.85%\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.98019\n",
      "675/675 [==============================] - 24s 36ms/step - loss: 0.0694 - accuracy: 0.9790 - val_loss: 0.0718 - val_accuracy: 0.9785\n",
      "Epoch 11/50\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0519 - accuracy: 0.9850 Epoch 11/50 -  Training Loss: 0.0518,  Validation Loss: 0.0814,  Training Accuracy: 98.50%,  Validation Accuracy: 97.35%\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.98019\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.0518 - accuracy: 0.9850 - val_loss: 0.0814 - val_accuracy: 0.9735\n",
      "Epoch 12/50\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9859 Epoch 12/50 -  Training Loss: 0.0482,  Validation Loss: 0.1396,  Training Accuracy: 98.59%,  Validation Accuracy: 96.22%\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.98019\n",
      "675/675 [==============================] - 24s 35ms/step - loss: 0.0482 - accuracy: 0.9859 - val_loss: 0.1396 - val_accuracy: 0.9622\n",
      "Epoch 13/50\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0546 - accuracy: 0.9841 Epoch 13/50 -  Training Loss: 0.0547,  Validation Loss: 0.2681,  Training Accuracy: 98.40%,  Validation Accuracy: 92.41%\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.98019\n",
      "675/675 [==============================] - 24s 36ms/step - loss: 0.0547 - accuracy: 0.9840 - val_loss: 0.2681 - val_accuracy: 0.9241\n",
      "Epoch 14/50\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9852 Epoch 14/50 -  Training Loss: 0.0505,  Validation Loss: 0.0610,  Training Accuracy: 98.52%,  Validation Accuracy: 98.30%\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.98019 to 0.98296, saving model to best_model.h5\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.0505 - accuracy: 0.9852 - val_loss: 0.0610 - val_accuracy: 0.9830\n",
      "Epoch 15/50\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0416 - accuracy: 0.9879 Epoch 15/50 -  Training Loss: 0.0416,  Validation Loss: 1.4392,  Training Accuracy: 98.79%,  Validation Accuracy: 70.76%\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.98296\n",
      "675/675 [==============================] - 23s 35ms/step - loss: 0.0416 - accuracy: 0.9879 - val_loss: 1.4392 - val_accuracy: 0.7076\n",
      "Epoch 16/50\n",
      "673/675 [============================>.] - ETA: 0s - loss: 0.0323 - accuracy: 0.9913 Epoch 16/50 -  Training Loss: 0.0324,  Validation Loss: 0.1581,  Training Accuracy: 99.13%,  Validation Accuracy: 95.74%\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.98296\n",
      "675/675 [==============================] - 24s 36ms/step - loss: 0.0324 - accuracy: 0.9913 - val_loss: 0.1581 - val_accuracy: 0.9574\n",
      "Epoch 17/50\n",
      "673/675 [============================>.] - ETA: 0s - loss: 0.0302 - accuracy: 0.9918 Epoch 17/50 -  Training Loss: 0.0303,  Validation Loss: 0.0943,  Training Accuracy: 99.18%,  Validation Accuracy: 97.59%\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.98296\n",
      "675/675 [==============================] - 24s 36ms/step - loss: 0.0303 - accuracy: 0.9918 - val_loss: 0.0943 - val_accuracy: 0.9759\n",
      "Epoch 18/50\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0289 - accuracy: 0.9916 Epoch 18/50 -  Training Loss: 0.0289,  Validation Loss: 0.1188,  Training Accuracy: 99.16%,  Validation Accuracy: 96.17%\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.98296\n",
      "675/675 [==============================] - 23s 35ms/step - loss: 0.0289 - accuracy: 0.9916 - val_loss: 0.1188 - val_accuracy: 0.9617\n",
      "Epoch 19/50\n",
      "673/675 [============================>.] - ETA: 0s - loss: 0.0225 - accuracy: 0.9931 Epoch 19/50 -  Training Loss: 0.0224,  Validation Loss: 0.0532,  Training Accuracy: 99.31%,  Validation Accuracy: 98.57%\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.98296 to 0.98574, saving model to best_model.h5\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.0224 - accuracy: 0.9931 - val_loss: 0.0532 - val_accuracy: 0.9857\n",
      "Epoch 20/50\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0357 - accuracy: 0.9894 Epoch 20/50 -  Training Loss: 0.0357,  Validation Loss: 0.1063,  Training Accuracy: 98.94%,  Validation Accuracy: 97.50%\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.98574\n",
      "675/675 [==============================] - 24s 35ms/step - loss: 0.0357 - accuracy: 0.9894 - val_loss: 0.1063 - val_accuracy: 0.9750\n",
      "Epoch 21/50\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0204 - accuracy: 0.9935 Epoch 21/50 -  Training Loss: 0.0204,  Validation Loss: 0.0721,  Training Accuracy: 99.34%,  Validation Accuracy: 98.26%\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.98574\n",
      "675/675 [==============================] - 23s 34ms/step - loss: 0.0204 - accuracy: 0.9934 - val_loss: 0.0721 - val_accuracy: 0.9826\n",
      "Epoch 22/50\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0170 - accuracy: 0.9952 Epoch 22/50 -  Training Loss: 0.0169,  Validation Loss: 0.1781,  Training Accuracy: 99.52%,  Validation Accuracy: 95.35%\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.98574\n",
      "675/675 [==============================] - 25s 36ms/step - loss: 0.0169 - accuracy: 0.9952 - val_loss: 0.1781 - val_accuracy: 0.9535\n",
      "Epoch 23/50\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0234 - accuracy: 0.9929 Epoch 23/50 -  Training Loss: 0.0233,  Validation Loss: 0.0791,  Training Accuracy: 99.29%,  Validation Accuracy: 98.22%\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.98574\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.0233 - accuracy: 0.9929 - val_loss: 0.0791 - val_accuracy: 0.9822\n",
      "Epoch 24/50\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9958 Epoch 24/50 -  Training Loss: 0.0141,  Validation Loss: 0.0892,  Training Accuracy: 99.58%,  Validation Accuracy: 98.17%\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.98574\n",
      "675/675 [==============================] - 24s 36ms/step - loss: 0.0141 - accuracy: 0.9958 - val_loss: 0.0892 - val_accuracy: 0.9817\n",
      "Epoch 25/50\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0176 - accuracy: 0.9943 Epoch 25/50 -  Training Loss: 0.0176,  Validation Loss: 0.1293,  Training Accuracy: 99.44%,  Validation Accuracy: 96.94%\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.98574\n",
      "675/675 [==============================] - 24s 36ms/step - loss: 0.0176 - accuracy: 0.9944 - val_loss: 0.1293 - val_accuracy: 0.9694\n",
      "Epoch 26/50\n",
      "673/675 [============================>.] - ETA: 0s - loss: 0.0098 - accuracy: 0.9974 Epoch 26/50 -  Training Loss: 0.0099,  Validation Loss: 0.0588,  Training Accuracy: 99.73%,  Validation Accuracy: 98.72%\n",
      "\n",
      "Epoch 26: val_accuracy improved from 0.98574 to 0.98722, saving model to best_model.h5\n",
      "675/675 [==============================] - 24s 36ms/step - loss: 0.0099 - accuracy: 0.9973 - val_loss: 0.0588 - val_accuracy: 0.9872\n",
      "Epoch 27/50\n",
      "673/675 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9973 Epoch 27/50 -  Training Loss: 0.0096,  Validation Loss: 0.0838,  Training Accuracy: 99.73%,  Validation Accuracy: 98.17%\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.98722\n",
      "675/675 [==============================] - 23s 35ms/step - loss: 0.0096 - accuracy: 0.9973 - val_loss: 0.0838 - val_accuracy: 0.9817\n",
      "Epoch 28/50\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9955 Epoch 28/50 -  Training Loss: 0.0150,  Validation Loss: 0.2899,  Training Accuracy: 99.55%,  Validation Accuracy: 92.20%\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.98722\n",
      "675/675 [==============================] - 24s 36ms/step - loss: 0.0150 - accuracy: 0.9955 - val_loss: 0.2899 - val_accuracy: 0.9220\n",
      "Epoch 29/50\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.9971 Epoch 29/50 -  Training Loss: 0.0083,  Validation Loss: 0.0722,  Training Accuracy: 99.71%,  Validation Accuracy: 98.39%\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.98722\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.0083 - accuracy: 0.9971 - val_loss: 0.0722 - val_accuracy: 0.9839\n",
      "Epoch 30/50\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9976 Epoch 30/50 -  Training Loss: 0.0069,  Validation Loss: 0.1643,  Training Accuracy: 99.76%,  Validation Accuracy: 96.81%\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.98722\n",
      "675/675 [==============================] - 23s 34ms/step - loss: 0.0069 - accuracy: 0.9976 - val_loss: 0.1643 - val_accuracy: 0.9681\n",
      "Epoch 31/50\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9955 Epoch 31/50 -  Training Loss: 0.0148,  Validation Loss: 0.0686,  Training Accuracy: 99.56%,  Validation Accuracy: 98.24%\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.98722\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.0148 - accuracy: 0.9956 - val_loss: 0.0686 - val_accuracy: 0.9824\n",
      "Epoch 32/50\n",
      "673/675 [============================>.] - ETA: 0s - loss: 0.0098 - accuracy: 0.9969 Epoch 32/50 -  Training Loss: 0.0098,  Validation Loss: 0.1372,  Training Accuracy: 99.69%,  Validation Accuracy: 96.61%\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.98722\n",
      "675/675 [==============================] - 23s 34ms/step - loss: 0.0098 - accuracy: 0.9969 - val_loss: 0.1372 - val_accuracy: 0.9661\n",
      "Epoch 33/50\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9966 Epoch 33/50 -  Training Loss: 0.0104,  Validation Loss: 0.0761,  Training Accuracy: 99.66%,  Validation Accuracy: 98.56%\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.98722\n",
      "675/675 [==============================] - 24s 35ms/step - loss: 0.0104 - accuracy: 0.9966 - val_loss: 0.0761 - val_accuracy: 0.9856\n",
      "Epoch 34/50\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0087 - accuracy: 0.9977 Epoch 34/50 -  Training Loss: 0.0086,  Validation Loss: 0.8613,  Training Accuracy: 99.77%,  Validation Accuracy: 81.81%\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.98722\n",
      "675/675 [==============================] - 26s 39ms/step - loss: 0.0086 - accuracy: 0.9977 - val_loss: 0.8613 - val_accuracy: 0.8181\n",
      "Epoch 35/50\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9979 Epoch 35/50 -  Training Loss: 0.0074,  Validation Loss: 0.5591,  Training Accuracy: 99.79%,  Validation Accuracy: 89.98%\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.98722\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.5591 - val_accuracy: 0.8998\n",
      "Epoch 36/50\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.9983 Epoch 36/50 -  Training Loss: 0.0067,  Validation Loss: 0.0794,  Training Accuracy: 99.83%,  Validation Accuracy: 98.39%\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.98722\n",
      "Restoring model weights from the end of the best epoch: 26.\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.0067 - accuracy: 0.9983 - val_loss: 0.0794 - val_accuracy: 0.9839\n",
      "Epoch 36: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "epochs = 50\n",
    "\n",
    "# Define the ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    \"best_model.h5\",\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback to stop training if there's no improvement in validation accuracy\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "print_loss_accuracy_callback = LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: print(\n",
    "        f\" Epoch {epoch + 1}/{epochs} - \"\n",
    "        f\" Training Loss: {logs['loss']:.4f}, \"\n",
    "        f\" Validation Loss: {logs['val_loss']:.4f}, \"\n",
    "        f\" Training Accuracy: {logs['accuracy'] * 100:.2f}%, \"\n",
    "        f\" Validation Accuracy: {logs['val_accuracy'] * 100:.2f}%\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Train the model using the generators and include the callbacks\n",
    "history = model.fit(train_gen, validation_data=test_gen, epochs=epochs, callbacks=[print_loss_accuracy_callback, checkpoint_callback, early_stopping_callback])\n",
    "\n",
    "# model.fit(train_gen, validation_data=test_gen, epochs=epochs, callbacks=[print_loss_accuracy_callback, checkpoint_callback])\n",
    "\n",
    "# Load the best model based on validation accuracy\n",
    "best_model = load_model(\"best_model.h5\", custom_objects={'Quantum': Quantum})\n",
    "\n",
    "# Save the best model with a different name, for example, \"PQC_Circuit\"\n",
    "best_model.save(\"Three_Categories_Quantum_best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa2698c5-3bc9-4cb6-b05b-548ccc2f9297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 6s 31ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      1100\n",
      "           1       0.98      1.00      0.99      2682\n",
      "           2       0.99      0.98      0.98      1618\n",
      "\n",
      "    accuracy                           0.99      5400\n",
      "   macro avg       0.99      0.98      0.99      5400\n",
      "weighted avg       0.99      0.99      0.99      5400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(test_gen)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce2e4b-a823-47dd-847b-9df15365847c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
