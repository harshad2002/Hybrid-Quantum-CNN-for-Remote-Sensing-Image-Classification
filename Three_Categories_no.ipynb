{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad0b3a4-f0f8-4b09-b0c9-cab5dc79a3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 13:03:35.993970: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-02 13:03:36.029503: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-02 13:03:36.029535: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-02 13:03:36.030391: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-02 13:03:36.036172: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-02 13:03:36.748578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from osgeo import gdal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pennylane as qml\n",
    "from keras.models import load_model\n",
    "from keras.utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout, Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "710bc194-c4d8-4839-87ef-7dc700778984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TiffImageDataGenerator(Sequence):\n",
    "    def __init__(self, image_files, labels, batch_size):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_files) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.image_files[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "\n",
    "        return np.array([\n",
    "            self.preprocess_image(file_name) for file_name in batch_x]), np.array(batch_y)\n",
    "\n",
    "    def preprocess_image(self, file):\n",
    "        dataset = gdal.Open(file)\n",
    "        channels = [dataset.GetRasterBand(i + 1).ReadAsArray() for i in range(dataset.RasterCount)]\n",
    "        image = np.stack(channels, axis=-1)\n",
    "        image = cv2.resize(image, (64, 64))\n",
    "        image = image / 255.0\n",
    "        return image\n",
    "\n",
    "dataset_path = \"/home/admin1/Selvin/BE/EuroSAT_MS/\"\n",
    "subdirs = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "image_files = []\n",
    "labels = []\n",
    "label = 0\n",
    "for subdir in subdirs:\n",
    "    image_files_subdir = [os.path.join(dataset_path, subdir, f) for f in os.listdir(os.path.join(dataset_path, subdir)) if f.endswith(\".tif\")]\n",
    "    image_files.extend(image_files_subdir)\n",
    "    labels.extend([label]*len(image_files_subdir))\n",
    "    label += 1\n",
    "\n",
    "# Define a dictionary to map the old labels to the new ones\n",
    "# Define a dictionary to map the old labels to the new ones\n",
    "label_mapping = {\n",
    "    'SeaLake': 'Water_Bodies',\n",
    "    'River': 'Water_Bodies',\n",
    "    'HerbaceousVegetation': 'Vegetation',\n",
    "    'PermanentCrop': 'Vegetation',\n",
    "    'AnnualCrop': 'Vegetation',\n",
    "    'Pasture': 'Vegetation',\n",
    "    'Forest': 'Vegetation',\n",
    "    'Industrial': 'Urban',\n",
    "    'Highway': 'Urban',\n",
    "    'Residential': 'Urban'\n",
    "}\n",
    "\n",
    "# Update the labels\n",
    "new_labels = []\n",
    "image_files = []  # Initialize the image_files list\n",
    "for subdir in subdirs:\n",
    "    image_files_subdir = [os.path.join(dataset_path, subdir, f) for f in os.listdir(os.path.join(dataset_path, subdir)) if f.endswith(\".tif\")]\n",
    "    image_files.extend(image_files_subdir)\n",
    "    new_labels.extend([label_mapping[subdir]]*len(image_files_subdir))\n",
    "\n",
    "# Now, you can use `new_labels` in place of `labels` for your train-test split and data generator\n",
    "# Define a dictionary to map the new string labels to numerical labels\n",
    "str_to_num_mapping = {\n",
    "    'Water_Bodies': 0,\n",
    "    'Vegetation': 1,\n",
    "    'Urban': 2\n",
    "}\n",
    "\n",
    "# Convert the string labels to numerical labels\n",
    "num_labels = [str_to_num_mapping[label] for label in new_labels]\n",
    "\n",
    "# Now, you can use `num_labels` in place of `new_labels` for your train-test split and data generator\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_files, num_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TiffImageDataGenerator instance for training and testing data\n",
    "train_gen = TiffImageDataGenerator(X_train, y_train, batch_size=32)\n",
    "test_gen = TiffImageDataGenerator(X_test, y_test, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79a4f57e-2860-47cb-b362-1466e1927762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantum_No(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Quantum_No, self).__init__(**kwargs)\n",
    "        self.dev = qml.device(\"default.qubit\", wires=4)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Quantum_No, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Convert inputs to tensor\n",
    "        inputs = tf.convert_to_tensor(inputs)\n",
    "        # Compute quantum circuit results\n",
    "        output = tf.vectorized_map(self.quantum_func, inputs)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 16)\n",
    "\n",
    "    def quantum_func(self, inputs):\n",
    "        @qml.qnode(self.dev, interface='tf')\n",
    "        def quantum_circuit(params):\n",
    "            qml.Hadamard(wires=0)\n",
    "            qml.Hadamard(wires=1)\n",
    "            qml.Hadamard(wires=2)\n",
    "            qml.Hadamard(wires=3)\n",
    "\n",
    "            # Use the values in the batch item for the initial rotation (theta1)\n",
    "            for i in range(4):\n",
    "                qml.RY(params[i], wires=i)\n",
    "\n",
    "            return qml.probs(wires=[0, 1, 2, 3])\n",
    "\n",
    "        # Compute quantum circuit results\n",
    "        probs = quantum_circuit(inputs)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bbea8a4-b919-4527-b157-bd4f8fb7658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 13)))  # Assuming your images have 3 channels\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Quantum_No())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(3, activation='softmax'))  # Assuming you have 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b11eb11-72d5-4cb3-a20e-719c214dc682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin1/miniconda3/lib/python3.11/site-packages/osgeo/gdal.py:312: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "WARNING:tensorflow:You are casting an input of type complex128 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 13:04:31.374132: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-04-02 13:04:31.500702: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:31.502024: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:225] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.6\n",
      "2024-04-02 13:04:31.502046: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:228] Used ptxas at ptxas\n",
      "2024-04-02 13:04:31.502093: W external/local_xla/xla/stream_executor/gpu/redzone_allocator.cc:322] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2024-04-02 13:04:31.717034: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:31.718408: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:31.811179: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:31.811448: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:31.811841: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:31.812216: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:31.812848: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:31.812903: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:31.813067: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:31.813546: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:31.813900: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:31.814632: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:31.814672: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:31.814843: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:31.815243: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:31.816050: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:31.816061: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:31.816123: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.012067: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.013637: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.166926: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.167331: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.167644: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.168521: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.169366: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.169538: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.169575: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.170481: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.170862: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.170950: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.172760: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.172788: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.173265: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.173554: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.174968: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.175037: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.175857: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.177243: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.703407: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.703882: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.704197: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.704731: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.705894: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.706172: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.706220: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.706584: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.707044: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.707445: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.707894: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.708015: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.708728: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:32.708837: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.708867: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.709319: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.709363: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:32.710035: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:33.119169: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:33.120765: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:33.159364: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:33.160848: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:33.206868: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:33.208376: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:33.245917: I external/local_xla/xla/service/service.cc:168] XLA service 0x7ff10b603ed0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-02 13:04:33.245940: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A4000, Compute Capability 8.6\n",
      "2024-04-02 13:04:33.250767: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-02 13:04:33.283223: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:33.284636: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:408] Couldn't read CUDA driver version.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712043273.301275   21775 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-04-02 13:04:33.335769: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:33.410266: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:33.449026: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:33.450568: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2024-04-02 13:04:33.559947: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:33.726401: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:33.855242: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:33.981990: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:34.084405: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:34.188045: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:34.356351: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:34.817570: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:34.938956: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:35.149322: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2024-04-02 13:04:35.251360: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/675 [..............................] - ETA: 1:15:58 - loss: 1.1772 - accuracy: 0.3438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 13:04:35.433585: E external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674/675 [============================>.] - ETA: 0s - loss: 0.4076 - accuracy: 0.8407 Epoch 1/30 -  Training Loss: 0.4071,  Validation Loss: 0.2344,  Training Accuracy: 84.09%,  Validation Accuracy: 93.39%\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.93389, saving model to best_model.h5\n",
      "675/675 [==============================] - 40s 50ms/step - loss: 0.4071 - accuracy: 0.8409 - val_loss: 0.2344 - val_accuracy: 0.9339\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin1/miniconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - ETA: 0s - loss: 0.1807 - accuracy: 0.9397 Epoch 2/30 -  Training Loss: 0.1807,  Validation Loss: 1.1481,  Training Accuracy: 93.97%,  Validation Accuracy: 69.15%\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.93389\n",
      "675/675 [==============================] - 27s 39ms/step - loss: 0.1807 - accuracy: 0.9397 - val_loss: 1.1481 - val_accuracy: 0.6915\n",
      "Epoch 3/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.1472 - accuracy: 0.9527 Epoch 3/30 -  Training Loss: 0.1472,  Validation Loss: 0.1538,  Training Accuracy: 95.27%,  Validation Accuracy: 94.41%\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.93389 to 0.94407, saving model to best_model.h5\n",
      "675/675 [==============================] - 25s 36ms/step - loss: 0.1472 - accuracy: 0.9527 - val_loss: 0.1538 - val_accuracy: 0.9441\n",
      "Epoch 4/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9628 Epoch 4/30 -  Training Loss: 0.1242,  Validation Loss: 0.1673,  Training Accuracy: 96.28%,  Validation Accuracy: 93.59%\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.94407\n",
      "675/675 [==============================] - 25s 37ms/step - loss: 0.1242 - accuracy: 0.9628 - val_loss: 0.1673 - val_accuracy: 0.9359\n",
      "Epoch 5/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.1089 - accuracy: 0.9669 Epoch 5/30 -  Training Loss: 0.1088,  Validation Loss: 0.2351,  Training Accuracy: 96.69%,  Validation Accuracy: 90.72%\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.94407\n",
      "675/675 [==============================] - 24s 36ms/step - loss: 0.1088 - accuracy: 0.9669 - val_loss: 0.2351 - val_accuracy: 0.9072\n",
      "Epoch 6/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.9716 Epoch 6/30 -  Training Loss: 0.0962,  Validation Loss: 0.2016,  Training Accuracy: 97.16%,  Validation Accuracy: 93.00%\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.94407\n",
      "675/675 [==============================] - 27s 40ms/step - loss: 0.0962 - accuracy: 0.9716 - val_loss: 0.2016 - val_accuracy: 0.9300\n",
      "Epoch 7/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0896 - accuracy: 0.9740 Epoch 7/30 -  Training Loss: 0.0896,  Validation Loss: 0.0799,  Training Accuracy: 97.40%,  Validation Accuracy: 97.74%\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.94407 to 0.97741, saving model to best_model.h5\n",
      "675/675 [==============================] - 26s 38ms/step - loss: 0.0896 - accuracy: 0.9740 - val_loss: 0.0799 - val_accuracy: 0.9774\n",
      "Epoch 8/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9785 Epoch 8/30 -  Training Loss: 0.0725,  Validation Loss: 0.2224,  Training Accuracy: 97.85%,  Validation Accuracy: 93.02%\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.97741\n",
      "675/675 [==============================] - 25s 38ms/step - loss: 0.0725 - accuracy: 0.9785 - val_loss: 0.2224 - val_accuracy: 0.9302\n",
      "Epoch 9/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9780 Epoch 9/30 -  Training Loss: 0.0721,  Validation Loss: 0.0756,  Training Accuracy: 97.80%,  Validation Accuracy: 97.67%\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.97741\n",
      "675/675 [==============================] - 27s 40ms/step - loss: 0.0721 - accuracy: 0.9780 - val_loss: 0.0756 - val_accuracy: 0.9767\n",
      "Epoch 10/30\n",
      "673/675 [============================>.] - ETA: 0s - loss: 0.0624 - accuracy: 0.9818 Epoch 10/30 -  Training Loss: 0.0624,  Validation Loss: 0.0803,  Training Accuracy: 98.18%,  Validation Accuracy: 97.09%\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.97741\n",
      "675/675 [==============================] - 28s 42ms/step - loss: 0.0624 - accuracy: 0.9818 - val_loss: 0.0803 - val_accuracy: 0.9709\n",
      "Epoch 11/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9818 Epoch 11/30 -  Training Loss: 0.0588,  Validation Loss: 0.2291,  Training Accuracy: 98.18%,  Validation Accuracy: 92.31%\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.97741\n",
      "675/675 [==============================] - 26s 38ms/step - loss: 0.0588 - accuracy: 0.9818 - val_loss: 0.2291 - val_accuracy: 0.9231\n",
      "Epoch 12/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0465 - accuracy: 0.9862 Epoch 12/30 -  Training Loss: 0.0465,  Validation Loss: 0.0541,  Training Accuracy: 98.62%,  Validation Accuracy: 98.39%\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.97741 to 0.98389, saving model to best_model.h5\n",
      "675/675 [==============================] - 26s 38ms/step - loss: 0.0465 - accuracy: 0.9862 - val_loss: 0.0541 - val_accuracy: 0.9839\n",
      "Epoch 13/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9834 Epoch 13/30 -  Training Loss: 0.0551,  Validation Loss: 1.6518,  Training Accuracy: 98.34%,  Validation Accuracy: 69.69%\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.98389\n",
      "675/675 [==============================] - 26s 39ms/step - loss: 0.0551 - accuracy: 0.9834 - val_loss: 1.6518 - val_accuracy: 0.6969\n",
      "Epoch 14/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9859 Epoch 14/30 -  Training Loss: 0.0439,  Validation Loss: 0.1028,  Training Accuracy: 98.59%,  Validation Accuracy: 96.76%\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.98389\n",
      "675/675 [==============================] - 28s 41ms/step - loss: 0.0439 - accuracy: 0.9859 - val_loss: 0.1028 - val_accuracy: 0.9676\n",
      "Epoch 15/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0309 - accuracy: 0.9915 Epoch 15/30 -  Training Loss: 0.0308,  Validation Loss: 0.0688,  Training Accuracy: 99.15%,  Validation Accuracy: 98.22%\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.98389\n",
      "675/675 [==============================] - 27s 40ms/step - loss: 0.0308 - accuracy: 0.9915 - val_loss: 0.0688 - val_accuracy: 0.9822\n",
      "Epoch 16/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9897 Epoch 16/30 -  Training Loss: 0.0320,  Validation Loss: 0.0788,  Training Accuracy: 98.97%,  Validation Accuracy: 96.81%\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.98389\n",
      "675/675 [==============================] - 30s 44ms/step - loss: 0.0320 - accuracy: 0.9897 - val_loss: 0.0788 - val_accuracy: 0.9681\n",
      "Epoch 17/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0264 - accuracy: 0.9918 Epoch 17/30 -  Training Loss: 0.0264,  Validation Loss: 0.0738,  Training Accuracy: 99.19%,  Validation Accuracy: 98.09%\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.98389\n",
      "675/675 [==============================] - 28s 41ms/step - loss: 0.0264 - accuracy: 0.9919 - val_loss: 0.0738 - val_accuracy: 0.9809\n",
      "Epoch 18/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9888 Epoch 18/30 -  Training Loss: 0.0340,  Validation Loss: 0.2095,  Training Accuracy: 98.88%,  Validation Accuracy: 93.07%\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.98389\n",
      "675/675 [==============================] - 28s 41ms/step - loss: 0.0340 - accuracy: 0.9888 - val_loss: 0.2095 - val_accuracy: 0.9307\n",
      "Epoch 19/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9954 Epoch 19/30 -  Training Loss: 0.0147,  Validation Loss: 0.1522,  Training Accuracy: 99.54%,  Validation Accuracy: 96.78%\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.98389\n",
      "675/675 [==============================] - 27s 40ms/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.1522 - val_accuracy: 0.9678\n",
      "Epoch 20/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9916 Epoch 20/30 -  Training Loss: 0.0256,  Validation Loss: 0.1953,  Training Accuracy: 99.16%,  Validation Accuracy: 95.65%\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.98389\n",
      "675/675 [==============================] - 29s 42ms/step - loss: 0.0256 - accuracy: 0.9916 - val_loss: 0.1953 - val_accuracy: 0.9565\n",
      "Epoch 21/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0198 - accuracy: 0.9941 Epoch 21/30 -  Training Loss: 0.0198,  Validation Loss: 0.0970,  Training Accuracy: 99.41%,  Validation Accuracy: 96.85%\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.98389\n",
      "675/675 [==============================] - 29s 43ms/step - loss: 0.0198 - accuracy: 0.9941 - val_loss: 0.0970 - val_accuracy: 0.9685\n",
      "Epoch 22/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9924 Epoch 22/30 -  Training Loss: 0.0258,  Validation Loss: 0.0611,  Training Accuracy: 99.24%,  Validation Accuracy: 98.50%\n",
      "\n",
      "Epoch 22: val_accuracy improved from 0.98389 to 0.98500, saving model to best_model.h5\n",
      "675/675 [==============================] - 27s 40ms/step - loss: 0.0258 - accuracy: 0.9924 - val_loss: 0.0611 - val_accuracy: 0.9850\n",
      "Epoch 23/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0144 - accuracy: 0.9956 Epoch 23/30 -  Training Loss: 0.0144,  Validation Loss: 0.0783,  Training Accuracy: 99.56%,  Validation Accuracy: 98.06%\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.98500\n",
      "675/675 [==============================] - 28s 42ms/step - loss: 0.0144 - accuracy: 0.9956 - val_loss: 0.0783 - val_accuracy: 0.9806\n",
      "Epoch 24/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0205 - accuracy: 0.9935 Epoch 24/30 -  Training Loss: 0.0205,  Validation Loss: 0.0713,  Training Accuracy: 99.35%,  Validation Accuracy: 98.07%\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.98500\n",
      "675/675 [==============================] - 27s 40ms/step - loss: 0.0205 - accuracy: 0.9935 - val_loss: 0.0713 - val_accuracy: 0.9807\n",
      "Epoch 25/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9972 Epoch 25/30 -  Training Loss: 0.0104,  Validation Loss: 0.0735,  Training Accuracy: 99.72%,  Validation Accuracy: 98.39%\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.98500\n",
      "675/675 [==============================] - 25s 38ms/step - loss: 0.0104 - accuracy: 0.9972 - val_loss: 0.0735 - val_accuracy: 0.9839\n",
      "Epoch 26/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0138 - accuracy: 0.9958 Epoch 26/30 -  Training Loss: 0.0138,  Validation Loss: 0.0603,  Training Accuracy: 99.58%,  Validation Accuracy: 98.80%\n",
      "\n",
      "Epoch 26: val_accuracy improved from 0.98500 to 0.98796, saving model to best_model.h5\n",
      "675/675 [==============================] - 26s 39ms/step - loss: 0.0138 - accuracy: 0.9958 - val_loss: 0.0603 - val_accuracy: 0.9880\n",
      "Epoch 27/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0144 - accuracy: 0.9954 Epoch 27/30 -  Training Loss: 0.0144,  Validation Loss: 0.3079,  Training Accuracy: 99.54%,  Validation Accuracy: 93.78%\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.98796\n",
      "675/675 [==============================] - 27s 40ms/step - loss: 0.0144 - accuracy: 0.9954 - val_loss: 0.3079 - val_accuracy: 0.9378\n",
      "Epoch 28/30\n",
      "674/675 [============================>.] - ETA: 0s - loss: 0.0195 - accuracy: 0.9939 Epoch 28/30 -  Training Loss: 0.0195,  Validation Loss: 0.1037,  Training Accuracy: 99.39%,  Validation Accuracy: 97.02%\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.98796\n",
      "675/675 [==============================] - 28s 41ms/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.1037 - val_accuracy: 0.9702\n",
      "Epoch 29/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9976 Epoch 29/30 -  Training Loss: 0.0082,  Validation Loss: 0.0537,  Training Accuracy: 99.76%,  Validation Accuracy: 98.81%\n",
      "\n",
      "Epoch 29: val_accuracy improved from 0.98796 to 0.98815, saving model to best_model.h5\n",
      "675/675 [==============================] - 30s 44ms/step - loss: 0.0082 - accuracy: 0.9976 - val_loss: 0.0537 - val_accuracy: 0.9881\n",
      "Epoch 30/30\n",
      "675/675 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9961 Epoch 30/30 -  Training Loss: 0.0126,  Validation Loss: 0.0862,  Training Accuracy: 99.61%,  Validation Accuracy: 98.35%\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.98815\n",
      "675/675 [==============================] - 27s 40ms/step - loss: 0.0126 - accuracy: 0.9961 - val_loss: 0.0862 - val_accuracy: 0.9835\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'QuantumNo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 37\u001b[0m\n\u001b[1;32m     32\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_gen, validation_data\u001b[38;5;241m=\u001b[39mtest_gen, epochs\u001b[38;5;241m=\u001b[39mepochs, callbacks\u001b[38;5;241m=\u001b[39m[print_loss_accuracy_callback, checkpoint_callback, early_stopping_callback])\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# model.fit(train_gen, validation_data=test_gen, epochs=epochs, callbacks=[print_loss_accuracy_callback, checkpoint_callback])\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Load the best model based on validation accuracy\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m best_model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, custom_objects\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantum_No\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mQuantumNo\u001b[49m})\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Save the best model with a different name, for example, \"PQC_Circuit\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m best_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThree_Categories_Quantum_No.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'QuantumNo' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "epochs = 30\n",
    "\n",
    "# Define the ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    \"best_model.h5\",\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback to stop training if there's no improvement in validation accuracy\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "print_loss_accuracy_callback = LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: print(\n",
    "        f\" Epoch {epoch + 1}/{epochs} - \"\n",
    "        f\" Training Loss: {logs['loss']:.4f}, \"\n",
    "        f\" Validation Loss: {logs['val_loss']:.4f}, \"\n",
    "        f\" Training Accuracy: {logs['accuracy'] * 100:.2f}%, \"\n",
    "        f\" Validation Accuracy: {logs['val_accuracy'] * 100:.2f}%\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Train the model using the generators and include the callbacks\n",
    "history = model.fit(train_gen, validation_data=test_gen, epochs=epochs, callbacks=[print_loss_accuracy_callback, checkpoint_callback, early_stopping_callback])\n",
    "\n",
    "# model.fit(train_gen, validation_data=test_gen, epochs=epochs, callbacks=[print_loss_accuracy_callback, checkpoint_callback])\n",
    "\n",
    "# Load the best model based on validation accuracy\n",
    "best_model = load_model(\"best_model.h5\", custom_objects={'Quantum_No': Quantum_No})\n",
    "\n",
    "# Save the best model with a different name, for example, \"PQC_Circuit\"\n",
    "best_model.save(\"Three_Categories_Quantum_No.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa2698c5-3bc9-4cb6-b05b-548ccc2f9297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 6s 34ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1100\n",
      "           1       0.98      0.99      0.99      2682\n",
      "           2       0.98      0.97      0.98      1618\n",
      "\n",
      "    accuracy                           0.98      5400\n",
      "   macro avg       0.98      0.98      0.98      5400\n",
      "weighted avg       0.98      0.98      0.98      5400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(test_gen)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce2e4b-a823-47dd-847b-9df15365847c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
